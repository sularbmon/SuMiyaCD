{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f669d5-b254-4061-a946-bdadd09de6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thithilab\\AppData\\Local\\Temp\\ipykernel_20300\\3037762796.py:78: FutureWarning: `multichannel` is a deprecated argument name for `hog`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  fd, hog_image = hog(resized_img, orientations=ORIENT, pixels_per_cell=PPC,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "# https://youtu.be/9GzfUzJeyi0\n",
    "\n",
    "\"\"\"\n",
    "@author: Sreenivas Bhattiprolu\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_classes=0  #to change when mee lay want to add more cows\n",
    "#images_per_validation=5\n",
    "\n",
    "\n",
    "\n",
    "#print(os.listdir(\"../Gold\"))\n",
    "#pixel per cell\n",
    "PPC = (8,8) #16 original\n",
    "#cells per block\n",
    "CPB = (2,2) # 4\n",
    "total_classes=0  \n",
    "#orientations\n",
    "ORIENT=9 #8\n",
    "#SIZE = 224\n",
    "IMG_SIZE2= 64\n",
    "IMG_SIZE1 = 128\n",
    "num_bins =4\n",
    "\n",
    "\n",
    "train_images = []\n",
    "train_labels = [] \n",
    "hog_train_images = []\n",
    "hog_train_features = []\n",
    "#height,width,layers=hog_train_images[1].shape\n",
    "#hog_train_video=cv2.VideoWriter('hog_trained_video.mp4', 0, 10, (IMG_SIZE1, IMG_SIZE2))\n",
    "#h = 0\n",
    "#w = 0\n",
    "for directory_path in glob.glob(\"D:\\LamenessData\\yolov8_features\\\\yolov8_segment_dataset\\\\Training/*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    #print(label)\n",
    "    total_classes+=1\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        #print(img_path)\n",
    "        #img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        \n",
    "        #img = cv2.resize(img, (IMG_SIZE1, IMG_SIZE2))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n",
    "        #train_images.append(img)\n",
    "        train_labels.append(label)\n",
    "        \n",
    "        #img = img[int(0.3*h):int(0.7*h),int(0.2*w):int(0.8*w)]\n",
    "        #fd,hog_image = hog(img, orientations=8, pixels_per_cell=(ppc,ppc),cells_per_block=(4, 4),block_norm= 'L2',visualise=True)\n",
    "        img = cv2.imread(img_path)\n",
    "    \n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        resized_img = resize(gray_image, (IMG_SIZE1, IMG_SIZE2))\n",
    "        #channel_r, channel_g, channel_b = cv2.split(resized_img)\n",
    "        \n",
    "        fd, hog_image = hog(resized_img, orientations=ORIENT, pixels_per_cell=PPC,\n",
    "                \tcells_per_block=CPB, visualize=True, multichannel=False)\n",
    "        #print(len(fd))\n",
    "        #fd,hog_image = hog(img, orientations=ORIENT, pixels_per_cell=(PPC,PPC),cells_per_block=(CPB,CPB),block_norm= 'L2',visualise=True)\n",
    "        #extract_hog_image = extract_hog(img, PPC, CPB, num_bins)\n",
    "        #hog_train_video.write(hog_image)\n",
    "        #cv2.imshow(\"Extracted Image\",fd)\n",
    "        #if(cv2.waitKey(1)==ord('a')):\n",
    "        #    raise stopIteration\n",
    "        #hog_train_images.append(hog_image)\n",
    "        hog_train_features.append(fd)\n",
    "\n",
    "#hog_train_video.release()        \n",
    "#train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "hog_train_images = np.array(hog_train_images)\n",
    "hog_train_features = np.array(hog_train_features)\n",
    "print(total_classes)\n",
    "\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ffb84-4e31-4d8b-a7a0-436ca9eaf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(hog_train_images), len(hog_train_features))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3916c841-8ca5-4905-8678-a6ffef0c4dd2",
   "metadata": {},
   "source": [
    "hog_train_video=cv2.VideoWriter('hog_trained_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 10, (IMG_SIZE1, IMG_SIZE2))\n",
    "#cv2.VideoWriter('hog_trained_video',cv2.VideoWriter_fourcc(*'mp4v'),10,(IMG_SIZE1,IMG_SIZE2))\n",
    "print(hog_train_images[1].shape)\n",
    "for j in range(len(hog_train_images)):\n",
    "    cv2.imshow(\"hog_images\",hog_train_images[j]);\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('a'):  # q to quit\n",
    "        raise StopIteration\n",
    "    hog_train_video.write(hog_train_images[j])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "hog_train_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7571ec9-6ec6-4988-96e9-34e42f05eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thithilab\\AppData\\Local\\Temp\\ipykernel_20300\\1592500512.py:25: FutureWarning: `multichannel` is a deprecated argument name for `hog`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  fd, hog_image = hog(resized_img, orientations=ORIENT, pixels_per_cell=PPC,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train lables encoded is here\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hog_test_images = []\n",
    "hog_test_features = []\n",
    "#test\n",
    "#test_images = []\n",
    "test_labels = [] \n",
    "images_per_validation =[]\n",
    "for directory_path in glob.glob(\"D:\\LamenessData\\yolov8_features\\\\yolov8_segment_dataset\\\\Validation/*\"):  #to change when baby add new images\n",
    "    fruit_label = directory_path.split(\"\\\\\")[-1]\n",
    "    count=0\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        #img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        #img = cv2.resize(img, (IMG_SIZE1, IMG_SIZE2))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) \n",
    "        #test_images.append(img)\n",
    "        test_labels.append(fruit_label)\n",
    "        #h,w = img.shape\n",
    "        #img = img[int(0.3*h):int(0.7*h),int(0.2*w):int(0.8*w)]\n",
    "        #fd,hog_image = hog(img, orientations=ORIENT, pixels_per_cell=(PPC,PPC),cells_per_block=(CPB,CPB),block_norm= 'L2',visualise=True)\n",
    "        img = cv2.imread(img_path)\n",
    "    \n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        resized_img = resize(gray_image, (IMG_SIZE1, IMG_SIZE2))\n",
    "        #channel_r, channel_g, channel_b = cv2.split(resized_img)\n",
    "        \n",
    "        fd, hog_image = hog(resized_img, orientations=ORIENT, pixels_per_cell=PPC,\n",
    "                \tcells_per_block=CPB, visualize=True, multichannel=False)\n",
    "        \n",
    "        #hog_test_images.append(hog_image)\n",
    "        hog_test_features.append(fd)\n",
    "        #print(len(fd))\n",
    "        count+=1\n",
    "    images_per_validation.append(count)\n",
    "        \n",
    "#test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "\n",
    "hog_test_images = np.array(hog_test_images)\n",
    "hog_test_features = np.array(hog_test_features)\n",
    "#Encode labels from text to integers.\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(test_labels)\n",
    "test_labels_encoded = le.transform(test_labels)\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded = le.transform(train_labels)\n",
    "print(\"Train lables encoded is here\")\n",
    "#print(train_labels_encoded)\n",
    "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15336732-0078-4fa8-a623-0273a0f9d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train, x_test, y_test = hog_train_features, train_labels_encoded, hog_test_features, test_labels_encoded\n",
    "#x_train, y_train, x_test, y_test = hog_train_images, train_labels_encoded, hog_test_images, test_labels_encoded\n",
    "\n",
    "###################################################################\n",
    "# Normalize pixel values to between 0 and 1\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#temp_x_train, nx, ny = x_train.shape\n",
    "#x_train = x_train.reshape((temp_x_train,nx*ny))\n",
    "\n",
    "#temp_y_train, nx, ny = y_train.shape\n",
    "#y_train = y_train.reshape((temp_y_train,nx*ny))\n",
    "\n",
    "\n",
    "#One hot encode y values for neural network. \n",
    "#from keras.utils import to_categorical\n",
    "#y_train_one_hot = to_categorical(y_train)\n",
    "#y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e53133-981a-4f27-bc64-7ce06f2b9268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is done\n"
     ]
    }
   ],
   "source": [
    "print(\"is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691d3ead-f90e-4fc9-8b20-da8ecfd634da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "general_x_train_name ='HOKAIDO_HOG_SVM_CHANNELS/GENERAL_X_TRAIN_GRAY.pkl'\n",
    "pickle.dump(x_train,open(general_x_train_name,'wb'))\n",
    "\n",
    "general_y_train_name ='HOKAIDO_HOG_SVM_CHANNELS/GENERAL_Y_TRAIN_GRAY.pkl'\n",
    "pickle.dump(y_train,open(general_y_train_name,'wb'))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b066de5-c9df-4e45-9361-56967751c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_filename = 'HOKAIDO_HOG_SVM_CHANNELS/Hog_label_encode.le'\n",
    "pickle.dump(le, open(le_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba8b015-9633-42fa-afd0-1cbcbe6273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "x_train = pickle.load(open('HOKAIDO_HOG_SVM_CHANNELS/GENERAL_X_TRAIN_GRAY.pkl','rb'))\n",
    "y_train = pickle.load(open('HOKAIDO_HOG_SVM_CHANNELS/GENERAL_Y_TRAIN_GRAY.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b4659d-5404-4c05-b0ab-dc21048db4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(random_state=21, tol=0.0005,max_iter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a75aa344-ac7a-4ce3-941e-e7d612054f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "done fitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thithilab\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=1500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('fitting')\n",
    "clf.fit(x_train,y_train)\n",
    "print('done fitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d1f238-af7d-4c27-a080-6a7e5696145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rf_filename ='HOKAIDO_HOG_SVM_CHANNELS/HOG_SVM_GRAY.pkl'\n",
    "pickle.dump(clf,open(rf_filename,'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e93dcc-e2f5-4a7d-9eed-41d2147bf905",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2612523229.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    RUN UNTILL ME TO GET HOG CHANNEL . DON\"T FORGET TO CHANGE  _R to other channel as well\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "RUN UNTILL ME TO GET HOG CHANNEL . DON\"T FORGET TO CHANGE  _R to other channel as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f76af-2372-48e4-a58d-96279ad300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x_test, nx, ny = x_test.shape\n",
    "x_test = x_test.reshape((temp_x_test,nx*ny))\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6ae04-273a-4bf8-8874-5a83c32e72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \"+str(accuracy_score(y_test, y_pred)))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e968fb-ea9e-42c8-94de-0abe63f9fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models\n",
    "\n",
    "import pickle\n",
    "#saving VGG and SVM trained models\n",
    "\n",
    "\n",
    "le_filename = 'May_Weights/Hog_General/Hog_label_encode.le'\n",
    "pickle.dump(le, open(le_filename, 'wb'))\n",
    "\n",
    "hog_trains_filename ='May_Weights/Hog_General/Hog_x_train_images.pkl'\n",
    "pickle.dump(x_train,open(hog_trains_filename,'wb'), protocol=4)\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3be045-e21f-4bc1-b3eb-6686e8c75524",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_ytrain_filename ='May_Weights/Hog_General/Hogy_train_images.pkl'\n",
    "pickle.dump(y_train,open(hog_ytrain_filename,'wb'), protocol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a8253-0b28-4e41-bc0b-9078d975c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_filename ='May_Weights/Hog_General/SVM/Hog_svm_black.pkl'\n",
    "pickle.dump(clf,open(svm_filename,'wb'), protocol=4)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35ce4bc6-73ab-445e-a0f1-0d7d22e760d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#############################\n",
    "#Load model wothout classifier/fully connected layers\n",
    "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
    "\n",
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "for layer in VGG_model.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "VGG_model.summary()  #Trainable parameters will be 0\n",
    "\n",
    "\n",
    "#Now, let us use features from convolutional network for RF\n",
    "feature_extractor=VGG_model.predict(x_train)\n",
    "\n",
    "features = feature_extractor.reshape(feature_extractor.shape[0], -1)\n",
    "\n",
    "X_for_RF = features #This is our X input to RF\n",
    "\n",
    "print(\"generated featured\")\n",
    "\n",
    "\n",
    "\n",
    "#############################\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e055a7d-dc9b-4d2e-9301-7ec571650070",
   "metadata": {},
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split( features, \n",
    "#                                                     labels, \n",
    "#                                                     test_size=0.30)\n",
    "                                         \n",
    "    \n",
    "clf = LinearSVC(random_state=20, tol=0.01,max_iter=300) #rs=10 maxiter=200\n",
    "#clf = CalibratedClassifierCV(clf)  #added\n",
    "clf.fit(X_for_RF, y_train)\n",
    "\n",
    "\n",
    "#print(x_test.shape)\n",
    "#x_test_svm=x_test.reshape(32786,256)\n",
    "#x_test_svm = (x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2] * x_test.shape[3]))\n",
    "#target_x = (target_x.reshape(target_x.shape[0], target_x.shape[1] * target_x.shape[2] * target_x.shape[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5059341-9b35-4aef-b7d1-cf92955f2e9a",
   "metadata": {},
   "source": [
    "X_test_feature = VGG_model.predict(x_test)\n",
    "X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)\n",
    "\n",
    "prediction_SVM = clf.predict(X_test_features)\n",
    "\n",
    "#print(prediction_SVM)\n",
    "# get the accuracy\n",
    "from sklearn import metrics\n",
    "#print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_SVM))\n",
    "#added to save prediction RF \n",
    "import pandas as pd\n",
    "pd.DataFrame(prediction_SVM, columns=['predictions']).to_csv('csv/prediction_SVM_RS72_100_cow_117_epochs.csv')\n",
    "\n",
    "#Print overall accuracy\n",
    "#from sklearn import metrics\n",
    "#print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_SVM))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "#cm = confusion_matrix(test_labels, prediction_SVM)\n",
    "#print(cm)\n",
    "#sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9c0b560-3c67-4f38-8c68-8b35acb0b369",
   "metadata": {},
   "source": [
    "#saving vgg and svm\n",
    "import pickle\n",
    "vgg_filename = 'vgg16_model.sav'\n",
    "#pickle.dump(VGG_model, open(vgg_filename, 'wb'))\n",
    "\n",
    "svm_filename ='svm.sav'\n",
    "#pickle.dump(clf,open(svm_filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b09e7c-7501-4528-a3dc-41db5c21e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modelsavedModels\n",
    "#vgg_filename = 'vgg16_model.sav'\n",
    "#svm_filename ='svm.sav'\n",
    "#VGG_model = pickle.load(open(vgg_filename, 'rb'))\n",
    "#clf = pickle.load(open(svm_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ed492-0342-4565-91cf-2be02cffeacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34cb07-e050-4e68-9f50-0d2513f48518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4b1d2-7456-4dcf-9199-7de099d2833b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "644361be-3126-4072-8cc9-2f204949cc0e",
   "metadata": {},
   "source": [
    "#testing OneClass VGG\n",
    "from sklearn.svm import OneClassSVM\n",
    "outlier_fraction = 0.028\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "known_images = []\n",
    "known_labels = [\"known\"]\n",
    "all_directory_path = \"D:\\\\Research\\\\Datasets\\\\cow_datasets_227_only\\\\all\\\\\"\n",
    "for img_path in glob.glob(os.path.join(all_directory_path, \"*.jpg\")):\n",
    "    #print(img_path)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    known_images.append(img)\n",
    "    \n",
    "print(len(known_images))\n",
    "    \n",
    "known_images = np.array(known_images)\n",
    "known_lables = np.array(known_labels)\n",
    "known_images = known_images / 255.0\n",
    "\n",
    "#print(known_images)\n",
    "\n",
    "VGG_model_OneClass = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
    "\n",
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "for layer in VGG_model_OneClass.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "VGG_model_OneClass.summary()  #Trainable parameters will be 0\n",
    "\n",
    "\n",
    "#Now, let us use features from convolutional network for RF\n",
    "oc_featureExtractor=VGG_model_OneClass.predict(known_images)\n",
    "\n",
    "oc_features = oc_featureExtractor.reshape(oc_featureExtractor.shape[0], -1)\n",
    "\n",
    "X_for_oneclass = oc_features #This is our X input to RF\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9d88d-25c3-4ac2-841b-89a4fdcf97f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "faf11fe2-7883-49d9-bf10-78dfe3e3e386",
   "metadata": {},
   "source": [
    "#testing IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# generate dataset\n",
    "#X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\t#n_clusters_per_class=1, weights=[0.999], flip_y=0, random_state=4)\n",
    "# split into train/test sets\n",
    "#trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "# define outlier detection model\n",
    "IsoF_model = IsolationForest(contamination=0.1)\n",
    "# fit on majority class\n",
    "#trainX = trainX[trainy==0]\n",
    "IsoF_model.fit(X_for_RF)\n",
    "# detect outliers in the test set\n",
    "#yhat = model.predict(testX)\n",
    "# mark inliers 1, outliers -1\n",
    "#testy[testy == 1] = -1\n",
    "#testy[testy == 0] = 1\n",
    "# calculate score\n",
    "#score = f1_score(testy, yhat, pos_label=-1)\n",
    "#print('F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90cbd003-dd2c-492c-9df8-78516719348e",
   "metadata": {},
   "source": [
    "def Isolation_Forest(img):\n",
    "    input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    oc_input_img_feature=VGG_model_OneClass.predict(input_img)\n",
    "    oc_input_img_features=oc_input_img_feature.reshape(oc_input_img_feature.shape[0], -1)\n",
    "    predicted_result = IsoF_model.predict(oc_input_img_features)[0]\n",
    "    #probs_svc = (decision - decision.min()) / (decision.max() - decision.min())\n",
    "    \n",
    "    #print(predicted_result)\n",
    "    #prediction =  IsoF_model.predict_proba(oc_input_img_features)\n",
    "    #print(\"decision function \" + str(decision))\n",
    "    print( \" prediction \"+ str(predicted_result))\n",
    "    return predicted_result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03883c4a-9e27-4a0f-a773-f82897d20d96",
   "metadata": {},
   "source": [
    "def Predict_SVM(image):  ## new with vgg\n",
    "    \n",
    "#Check results on a few select images\n",
    "    #n=np.random.randint(0, x_test.shape[0])\n",
    "    img = image\n",
    "\n",
    "    plt.imshow(img)\n",
    "    input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    input_img_feature=VGG_model.predict(input_img)\n",
    "    input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\n",
    "    prediction_RF = clf.predict(input_img_features)[0] \n",
    "    prediction_RF = le.inverse_transform([prediction_RF])  #Reverse the label encoder to original name\n",
    "    print(\"The prediction for this image is: \", prediction_RF)\n",
    "    #predict_proba= clf.predict_proba(input_img_features)\n",
    "    #print(predict_proba)\n",
    "    #print(\"max predict value is \"+str(predict_proba.max()) )\n",
    "    \n",
    "    #decision_svc= clf.predict_proba(input_img_features)\n",
    "    #probs_svc = (decision_svc - decision_svc.min()) / (decision_svc.max() - decision_svc.min())\n",
    "    #print(\"decision probability \"+str(probs_svc))\n",
    "    \n",
    "    return prediction_RF\n",
    "print(\"defined RF new VGG SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc708d6-084b-4674-86ac-648b9ebe0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hog_svm(img):\n",
    "    h,w,c = img.shape\n",
    "    #img = img[int(0.3*h):int(0.7*h),int(0.2*w):int(0.8*w)]\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) #open when need\n",
    "    img=img / 255.0\n",
    "    \n",
    "    #temp_img, nx, ny = img.shape\n",
    "    #img = img.reshape((temp_img,nx*ny))\n",
    "    #cv2.imshow('cropped region', imutils.resize(img, width = 1080,height=720))\n",
    "    \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    #img=img.reshape(img.shape[0], -1)\n",
    "    #fd,hog_image = hog(img, orientations=8, pixels_per_cell=(ppc,ppc),cells_per_block=(4, 4),block_norm= 'L2',visualise=True)\n",
    "    fd,hog_image = hog(img, orientations=ORIENT, pixels_per_cell=(PPC,PPC),cells_per_block=(CPB,CPB),block_norm= 'L2',visualise=True)\n",
    "    #print(hog_image.shape)\n",
    "    #cv2.imshow('cropped region',hog_image)\n",
    "    #if cv2.waitKey(1) == ord('a'):  # q to quit\n",
    "    #    raise StopIteration\n",
    "    #hog_test_images.append(hog_image)\n",
    "    hog_image = hog_image.ravel()\n",
    "    #hog_test_features.append(fd)\n",
    "    #input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    #input_img_feature=VGG_model.predict(input_img)\n",
    "    hog_image=hog_image.reshape(-1,hog_image.shape[0] )\n",
    "    #fd=fd.reshape(fd.shape[0],-1)\n",
    "    print(fd)\n",
    "    prediction = clf.predict(hog_image)\n",
    "    prediction = le.inverse_transform([prediction])  #Reverse the label encoder to original name\n",
    "    print (prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15969593-ce4b-4747-8d64-33ca462c5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STORED_IDS= []\n",
    "STORED_MID_Y = []\n",
    "STORED_MISS = []\n",
    "PREVIOUS_ID = [] # keep the record of last seen ids and position\n",
    "PREVIOUS_Y = [] \n",
    "PREVIOUS_LOCAL_IDS = []\n",
    "LOCAL_ID= 1000\n",
    "\n",
    "def Take_Prev_Label(y,h,id,cow_srno):\n",
    "    global STORED_IDS\n",
    "    global STORED_MID_Y\n",
    "    global STORED_MISS\n",
    "    global LAST_SEEN_IDS\n",
    "    global LAST_SEEN_ID_CENTROIDS\n",
    "    global LOCAL_ID\n",
    "    \n",
    "    mid_y = int(2*y + h)/2\n",
    "    IS_NEW = True\n",
    "    #print('cow_srno',cow_srno,' original id ',id)\n",
    "    #updatedID = Is_Duplicate_Id(mid_y,id)\n",
    "    #if(updatedID!=id):\n",
    "    #    print('orgID: ',id,' updated ID: ',updatedID)\n",
    "    #id=updatedID\n",
    "    #clear old records \n",
    "    if(len(STORED_IDS)>0):\n",
    "        #print('removed ',STORED_IDS[0])\n",
    "        #del STORED_MISS[0]\n",
    "        #del STORED_MID_Y[0]\n",
    "        \n",
    "        #del STORED_IDS[0]\n",
    "        #STORED_MISS= STORED_MISS[1:]\n",
    "        #STORED_MID_Y= STORED_MID_Y[1:]\n",
    "        #STORED_IDS= STORED_IDS[1:]\n",
    "        #print(STORED_MISS)\n",
    "        MISSED_LEN = len(STORED_MISS)\n",
    "        #if(IS_NEW):\n",
    "        \n",
    "        #    MISSED_LEN -=1\n",
    "        removed = 0\n",
    "        for i in range(MISSED_LEN):\n",
    "            #print(i, ' missed index checking' )\n",
    "            missed = STORED_MISS[i-removed]\n",
    "            #print('checking ',i-removed, 'to remove')\n",
    "            if(missed>10 and len(STORED_MISS)>0): #if missed 35 frames\n",
    "                print('removed ID',STORED_IDS[i-removed])\n",
    "                print('removed MID_Y',STORED_MID_Y[i-removed])\n",
    "                print('removed STORED_ID',STORED_IDS[i-removed])\n",
    "                del STORED_MISS[i-removed]  \n",
    "                del STORED_MID_Y[i-removed]\n",
    "                del STORED_IDS[i-removed]\n",
    "                removed+=1\n",
    "                #print('removed')\n",
    "                \n",
    "    #clear misses\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(1,len(STORED_MID_Y)+1):\n",
    "        #print(STORED_IDS[-i-1],STORED_MID_Y[-i-1],' ',i)\n",
    "        if(STORED_MID_Y[-i]+220>=mid_y and STORED_MID_Y[-i]-220<=mid_y): # and IS_NEW): #previous 150 #200\n",
    "            if(IS_NEW):\n",
    "                print('mid_y ',mid_y,'existing y ',STORED_MID_Y[-i])\n",
    "                print('all mid_y ',STORED_MID_Y) \n",
    "                IS_NEW = False\n",
    "                STORED_MID_Y[-i] = mid_y\n",
    "                STORED_MISS[-i]=1\n",
    "                id= STORED_IDS[-i]\n",
    "            \n",
    "            #try:\n",
    "            #    exist_index = LAST_SEEN_IDS.index(id)\n",
    "            #    if(LAST_SEEN_ID_CENTROIDS[exist_index]+200>y): # showing old id\n",
    "            #        LAST_SEEN_ID_CENTROIDS[exist_index] = y\n",
    "            #except:\n",
    "            #print('corrected id :',STORED_IDS[-i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "        #elif(cow_srno==1):\n",
    "        else:\n",
    "            STORED_MISS[-i]+=1\n",
    "                \n",
    "                \n",
    "    #print(STORED_IDS,' IDS ',STORED_MID_Y,' SMY ',mid_y,' mid_y')\n",
    "    if(IS_NEW):\n",
    "        print('SMY: ',STORED_MID_Y,', new my:',mid_y) \n",
    "        print('new id: ',id)\n",
    "        STORED_IDS.append(id)\n",
    "        STORED_MID_Y.append(mid_y)\n",
    "        STORED_MISS.append(1)\n",
    "    \n",
    "    print('returned id :',id)\n",
    "    return id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4bb81-65fc-4aa4-a41e-08a2f48ad939",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%python --source \"D:\\Python\\env\\Lameness\\Frames\\Videos\\20220201_145508_7108.mp4\"  --yolo-weights weights_slm/best_6_23_gpu.pt --view-img --save-crop --device 0\n",
    "\n",
    "\n",
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Run inference on images, videos, directories, streams, etc.\n",
    "\n",
    "Usage - sources:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam\n",
    "                                                             img.jpg        # image\n",
    "                                                             vid.mp4        # video\n",
    "                        \n",
    "                        path/          # directory\n",
    "                                                             path/*.jpg     # glob\n",
    "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "\n",
    "Usage - formats:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch\n",
    "                                         yolov5s.torchscript        # TorchScript\n",
    "                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                         yolov5s.xml                # OpenVINO\n",
    "                                         yolov5s.engine             # TensorRT\n",
    "                                         yolov5s.mlmodel            # CoreML (macOS-only)\n",
    "                                         yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                         yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                         yolov5s.tflite             # TensorFlow Lite\n",
    "                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import imutils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "FILE = Path(\"__file__\").resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "print(ROOT)\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "X1=240\n",
    "X2=400\n",
    "Y1=94\n",
    "Y2=590\n",
    "\n",
    "\n",
    "default=640\n",
    "save_video=True\n",
    "#file_location=\"C:\\\\Users\\\\thithilab\\\\Desktop\\\\20220706\\\\e_videos_6_7\" #file location  #done e_videos_6_7  #m_videos_6_7\n",
    "file_location=\"D:\\\\815_CowDataChecking\\\\20220907\\\\07_09_M\\\\\"\n",
    "filename=\"20220705_135955_4D30\"\n",
    "#Y1_NEW=110\n",
    "#Y2_NEW=530\n",
    "Y1_NEW=130\n",
    "Y2_NEW=510\n",
    "HAS_COW=False  # to save video when has cow\n",
    "\n",
    "cow_order=[]\n",
    "cow_count = []\n",
    "cow_label=[]\n",
    "frame_rate=10\n",
    "\n",
    "\n",
    "all_detected_cow=[]\n",
    "\n",
    "local_id=1\n",
    "\n",
    "def DoROI(image):\n",
    "    h,w,c = image.shape\n",
    "    img_arr = np.array(image)\n",
    "    img_arr[0 : int(94*(h/default)), 0 : h] = (0, 0, 0)   #top\n",
    "    img_arr[0 : h, 0 : int(240*(w/default))] = (0, 0, 0)   #left\n",
    "    img_arr[0 : h, int(400*(w/default)) : w] = (0, 0, 0)   #right\n",
    "    img_arr[int(590*(h/default)) : h,0 : w] = (0, 0, 0)   #bottom\n",
    "    return img_arr\n",
    "\n",
    "\n",
    "def DoROI_640(image):\n",
    "    img_arr = np.array(image)\n",
    "    img_arr[0 : 94, 0 : 640] = (0, 0, 0)   #top\n",
    "    img_arr[0 : 640, 0 : 240] = (0, 0, 0)   #left\n",
    "    img_arr[0 : 640, 400 : 640] = (0, 0, 0)   #right\n",
    "    img_arr[590 : 640,0 : 640] = (0, 0, 0)   #bottom\n",
    "    return img_arr\n",
    "  \n",
    "def check_withinROI(x1,y1,x2,y2,h,w):\n",
    "    if(x1<int(X1*(w/default)) or x2>int(X2*(w/default)) or y1<int(Y1*(h/default)) or y2>int(Y2*(h/default)) or x1>=int(X2*(w/default))):\n",
    "      return False\n",
    "    return True  \n",
    "\n",
    "def check_withinROI_NEW(x1,y1,x2,y2,h,w):\n",
    "    if(x1<int(X1*(w/default)) or x2>int(X2*(w/default)) or y1<int(Y1_NEW*(h/default)) or y2>int(Y2_NEW*(h/default)) or x1>=int(X2*(w/default))):\n",
    "      return False\n",
    "    return True  \n",
    "\n",
    "def check_cow_Count(label):\n",
    "    global cow_label\n",
    "    global cow_count\n",
    "    print(\"inserting label\")\n",
    "    if label in cow_label: #check exist\n",
    "        cow_count[cow_label.index(label)]+=1  #start counting of the newly inserted cow\n",
    "        \n",
    "    else:\n",
    "        cow_label.append(label)  # if not exist then add the cow label to array\n",
    "        cow_count.append(1)  #start counting of the newly inserted cow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def determine_label(img):\n",
    "    \n",
    "    #if Isolation_Forest(img) != 1:\n",
    "    #    res = ['unknown']\n",
    "    #    check_cow_Count(res[0])\n",
    "    #    return res\n",
    "    global all_detected_cow\n",
    "    label = Predict_SVM(img)\n",
    "    HAS_COW=True\n",
    "    check_cow_Count(label[0])\n",
    "    all_detected_cow.append(label[0])\n",
    "    return label\n",
    "        \n",
    "prev_labels=[]  #keep last records to compare y pixel value    \n",
    "prev_y1s=[]    \n",
    "\n",
    "\n",
    "def compare_with_prev_cow(label,y,h):\n",
    "    prev_labels.append(label)\n",
    "    prev_y1s.append(y)\n",
    "    has_100_record = len(prev_labels)\n",
    "    start = 0\n",
    "    end = 0\n",
    "    ceiling = h-int(h*(Y1_NEW/default))\n",
    "    #print(ceiling )\n",
    "    #print(h)\n",
    "    #print(y)\n",
    "    if(y+100>=ceiling) :   #checking if the image reach the top\n",
    "        if has_100_record>=20:\n",
    "            start=has_100_record - 20 - 1 #only check last 20 values\n",
    "            end = has_100_record - 1\n",
    "        cow_count_c=[]\n",
    "        cow_label_c=[]\n",
    "        prev_y_value=y\n",
    "        total_frames=0\n",
    "        global cow_order\n",
    "        for i in range(end,start,-1):\n",
    "            if(prev_y1s[i]>=h/2 +50 ):  # check only for half of screen\n",
    "                #for l in range(len(label[i].split(',')):\n",
    "                #split_label = label[i].split(',')[l]\n",
    "                #if split_label in cow_label: #check exist\n",
    "                if(prev_y1s[i]>prev_y_value):\n",
    "                    prev_y_value=prev_y1s[i] #go with 30 pixel different\n",
    "                    total_frames += 1\n",
    "                    if prev_labels[i] in cow_label_c:\n",
    "                        cow_count_c[cow_label_c.index(prev_labels[i])]+=1  #start counting of the newly inserted cow\n",
    "        \n",
    "                    else:\n",
    "                        cow_label_c.append(prev_labels[i])  # if not exist then add the cow label to array\n",
    "                        cow_count_c.append(1)  #start counting of the newly inserted cow\n",
    "                #else:\n",
    "                    \n",
    "                \n",
    "        #prediction_RF = np.argmax(prop)         \n",
    "        #get max cow id\n",
    "        if(len(cow_count_c)<1):\n",
    "            return None\n",
    "        max_count = max(cow_count_c)\n",
    "        threshold_50_percent = math.floor(total_frames*0.5)\n",
    "        if(max_count>threshold_50_percent + 1):\n",
    "            index = np.argmax(cow_count_c)\n",
    "            cow_order.append(cow_label_c[index])\n",
    "            print(\" cow label \"+str(cow_label_c[index]))\n",
    "            return cow_label_c[index]\n",
    "        else:\n",
    "            print(\" cow label unknown\")\n",
    "            cow_order.append(\"unknown\")\n",
    "            return \"unknown\"\n",
    "    \n",
    "    if(len(prev_y1s) >700): # delete first 500 when greater than 800\n",
    "        del prev_labels[:500]\n",
    "        del prev_y1s[:500]    \n",
    "         \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(\n",
    "    \n",
    "        #weights=ROOT / 'June_Weight/best.pt',  # model.pt path(s)\n",
    "        weights=ROOT / 'Sept_no_alien_weight_v1/best.pt',\n",
    "        source=ROOT / file_location,  # file/dir/URL/glob, 0 for webcam\n",
    "        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n",
    "        imgsz=(640, 640),  # inference size (height, width)\n",
    "        conf_thres=0.75,  # confidence threshold\n",
    "        iou_thres=0.45,  # NMS IOU threshold\n",
    "        max_det=1000,  # maximum detections per image\n",
    "        device='0',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        view_img=True,  # show results\n",
    "        save_txt=False,  # save results to *.txt                    \n",
    "        save_conf=False,  # save confidences in --save-txt labels\n",
    "        save_crop=False,  # save cropped prediction boxes\n",
    "        nosave=False,  # do not save images/videos\n",
    "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "        agnostic_nms=False,  # class-agnostic NMS\n",
    "        augment=False,  # augmented inference\n",
    "        visualize=False,  # visualize features\n",
    "        update=False,  # update all models\n",
    "        project=ROOT / 'runs/detect_hog',  # save results to project/name\n",
    "        name='exp_'+str(frame_rate)+'_fps',  # save results to project/name\n",
    "        exist_ok=False,  # existing project/name ok, do not increment\n",
    "        line_thickness=8,  # bounding box thickness (pixels)\n",
    "        hide_labels=False,  # hide labels\n",
    "        hide_conf=False,  # hide confidences\n",
    "        half=False,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "    \n",
    "):\n",
    "    \n",
    "    global all_detected_cow\n",
    "    global frame_rate\n",
    "    #added\n",
    "    sec=0\n",
    "    global cow_lable\n",
    "    global cow_count\n",
    "    global cow_order\n",
    "    cow_id = []\n",
    "    cow_id_original = []\n",
    "    cow_top = []\n",
    "    cow_left = []\n",
    "    cow_width = []\n",
    "    cow_height = []\n",
    "    cow_score = []\n",
    "    cow_frame = []\n",
    "    cow_filename= []\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    cf = 0  \n",
    "    count=0\n",
    "    read_after_frame=1\n",
    "    \n",
    "    source = str(source)\n",
    "    #save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "    #added\n",
    "    save_img=True\n",
    "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
    "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Directories\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    csv_save_dir = str(save_dir)\n",
    "    print(csv_save_dir)\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Dataloader\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = len(dataset)  # batch_size\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = 1  # batch_size\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "    #frame_rate = 4    #frame rate here\n",
    "    prev = 0\n",
    "    prev_frame= 0\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        #print(path)\n",
    "        #cv2.waitKey(1000) #1 fps   1000/ value =fps\n",
    "        #vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000) \n",
    "        #vid_cap.set(cv2.CV_CAP_PROP_FPS, 1)\n",
    "        #vid_cap.set(cv2.CAP_PROP_FPS, 1)\n",
    "        HAS_COW=False\n",
    "        t1 = time_sync()\n",
    "        im = torch.from_numpy(im).to(device)\n",
    "        im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "        t2 = time_sync()\n",
    "        dt[0] += t2 - t1\n",
    "        \n",
    "        # Inference\n",
    "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "        pred = model(im, augment=augment, visualize=visualize)\n",
    "        t3 = time_sync()\n",
    "        dt[1] += t3 - t2\n",
    "\n",
    "        # NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "        dt[2] += time_sync() - t3\n",
    "        \n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Process predictions\n",
    "        time_elapsed = time.time() - prev\n",
    "    \n",
    "\n",
    "        if (read_after_frame-prev_frame == 0):\n",
    "            prev_frame=0\n",
    "            continue\n",
    "        prev_frame +=1\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "           \n",
    "            \n",
    "            #print(time_elapsed)\n",
    "            seen += 1\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "                s += f'{i}: '\n",
    "            else:\n",
    "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "            \n",
    "            #added ROI    \n",
    "            #h,w,c=im0.shape\n",
    "            \n",
    "            #resize\n",
    "            #if(w>640 or h>640):\n",
    "            #  im0=imutils.resize(im0, width = 640)\n",
    "              \n",
    "\n",
    "            #ROI\n",
    "            im0=DoROI(im0)\n",
    "            h,w,c=im0.shape\n",
    "            \n",
    "            \n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # im.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "            s += '%gx%g ' % im.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                cow_position = 0\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if(check_withinROI_NEW(xyxy[0],xyxy[1],xyxy[2],xyxy[3],h,w)):\n",
    "                        #print(\"valid image\")\n",
    "                        count+=1\n",
    "                        box_left = xyxy[0]\n",
    "                        box_top = xyxy[1]\n",
    "                        box_w = xyxy[2] - xyxy[0]\n",
    "                        box_h = xyxy[3] - xyxy[1]\n",
    "                        #cow_score.append(conf)\n",
    "                        cow_frame.append(seen)\n",
    "                        cow_filename.append(str(p.stem)+'_'+str(count)+'.jpg')\n",
    "\n",
    "                        #feed on cnn and get label\n",
    "                        #save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}_{count}.jpg', BGR=True)\n",
    "\n",
    "                        #crop\n",
    "                        BGR=False\n",
    "                        #print(\"step-3-y\")\n",
    "                        crop = im0[int(xyxy[1]):int(xyxy[3]), int(xyxy[0]):int(xyxy[2])]\n",
    "                        #img = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR) #open when need\n",
    "                        #img=img / 255.0\n",
    "                        #temp_img, nx, ny = img.shape\n",
    "                        \n",
    "                        #img = img.reshape((temp_img,nx*ny))\n",
    "                        #print(img.shape)\n",
    "                        #cropped = torch.tensor(crop, device = 'cpu')\n",
    "                        #image = Image.fromarray(crop)\n",
    "                        #img = image.resize((128, 128), Image.ANTIALIAS)\n",
    "                        \n",
    "                        #do some process like testing data in cnn\n",
    "                        #img = cv2.resize(crop, (SIZE, SIZE)) #open when need\n",
    "                        #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #open when need\n",
    "                        #cv2.imshow('detected cow',img)\n",
    "                        #if cv2.waitKey(1) == ord('a'):  # q to quit\n",
    "                        #    raise StopIteration\n",
    "                        #crop=imutils.resize(crop, width = 224)\n",
    "                        # label = Predict_SVM(img)\n",
    "                        #label = determine_label(img) #first frame\n",
    "                        img = cv2.resize(crop, (IMG_SIZE1, IMG_SIZE2))\n",
    "                        label = predict_hog_svm(img)\n",
    "                        HAS_COW=True\n",
    "                        prev_id = Take_Prev_Label(box_top,box_h,label,cow_position)\n",
    "                        cow_position+=1\n",
    "                        HAS_COW=True\n",
    "                        \n",
    "                        cow_id_original.append(int(label[0]))\n",
    "                        cow_id.append(prev_id[0])\n",
    "                    \n",
    "                        annotator.box_label(xyxy, prev_id[0], color=colors(c, True))\n",
    "                        #label = Predict_SVM_test_pro(img)\n",
    "\n",
    "                        #check cow count here\n",
    "                        #h,w,c=im0.shape  \n",
    "                        #final_label = compare_with_prev_cow(label[0],int(xyxy[3]),h)\n",
    "                        #if final_label != None: print(\"final label \"+ final_label)\n",
    "                        #annotator.box_label(xyxy, label[0], color=(15, 0, 255))#color=colors(c, True))\n",
    "                        #if save_txt:  # Write to file\n",
    "                        #    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        #    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                        #    with open(f'{txt_path}.txt', 'a') as f:\n",
    "                        #        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "                        #if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        #    c = int(cls)  # integer class\n",
    "                        #    #label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')  #original\n",
    "                        #    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                        #if save_crop:\n",
    "                        #    #save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "                        \n",
    "\n",
    "            # Stream results\n",
    "            im0 = annotator.result()\n",
    "            if view_img:\n",
    "                \n",
    "                if(w>1080 or h>1080):\n",
    "                    cv2.imshow('detected cow', imutils.resize(im0, width = 1080,height=720))\n",
    "                else:\n",
    "                    cv2.imshow('detected cow',im0)\n",
    "                if cv2.waitKey(1) == ord('a'):  # q to quit\n",
    "                    raise StopIteration\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if (save_img or save_video) and HAS_COW:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path[i] != save_path:  # new video\n",
    "                        vid_path[i] = save_path\n",
    "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                            vid_writer[i].release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                            fps=frame_rate\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                            #fps=frame_rate\n",
    "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                        all_detected_cow.append('xxxxxxxxxxxxx')\n",
    "                        all_detected_cow.append('xxxxxxxxxxxxx')\n",
    "                        all_detected_cow.append(save_path)\n",
    "                        \n",
    "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer[i].write(im0)\n",
    "\n",
    "        # Print time (inference-only)\n",
    "        LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
    "    cv2.destroyAllWindows()    \n",
    "    df = pd.DataFrame(cow_id, columns = [\"ID\"])\n",
    "    \n",
    "    try:\n",
    "        original_ids = torch.tensor(cow_id_original, device = 'cpu')\n",
    "        df[\"Original\"] = original_ids\n",
    "    except:\n",
    "         df[\"Original\"] = cow_id_original\n",
    "    \n",
    "    now=str(datetime.now().date())\n",
    "    try:\n",
    "        print(all_detected_cow)\n",
    "        #all_detected_cow = torch.tensor(all_detected_cow,device=\"cpu\")\n",
    "        detected_cow_df = pd.DataFrame(all_detected_cow, columns = ['ID'])\n",
    "        detected_cow_df.to_csv('csv/all_detected_cow_'+str(frame_rate)+'_fps_'+now+'.csv')    \n",
    "    except :\n",
    "        print (\"couldn't save all_detected_cow\")\n",
    "    \n",
    "    \n",
    "    df.to_csv(csv_save_dir+'/detected_cow_RF_'+now+'.csv', index= False) \n",
    "    # Print results\n",
    "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
    "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
    "    \n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "    if update:\n",
    "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
    "\n",
    "    \n",
    "    print(cow_label)\n",
    "    print(cow_order)\n",
    "    print(cow_count)\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        cow_count_csv = pd.DataFrame(cow_label,columns=[\"ID\"])\n",
    "        cow_count = torch.tensor(cow_count,device='cpu')\n",
    "        cow_count_csv[\"count\"]=cow_count\n",
    "        cow_count_csv.to_csv('csv/cow_count_lable_'+now+'.csv',index=False)\n",
    "    except :\n",
    "        print(\"couldn't saved cow_count_\")\n",
    "    try:\n",
    "        \n",
    "        #cow_order = torch.tensor(cow_order , device='cpu')\n",
    "        cow_order_pd = pd.DataFrame(cow_order, columns=[\"Label\"])\n",
    "        cow_order_pd.to_csv('csv/cow_order_IsolationForest'+now+'.csv',index=False)\n",
    "    except :\n",
    "        print(\"couldn't saved cow_order\")\n",
    "    try:\n",
    "        \n",
    "        df_cows_in_frame = pd.DataFrame(cow_label,columns=[\"Label\"])\n",
    "        df_cows_in_frame[\"Total Frames\"]=torch.tensor(cow_count, device = 'cpu')\n",
    "        df_cows_in_frame.to_csv('csv/detecting_vggrf'+filename+'.mp4'+now+'.csv', index= False)\n",
    "    except :\n",
    "        print(\"Couldn't saved detecting_vggrf\")\n",
    "def parse_opt():\n",
    "    class Args:\n",
    "        weights='Sept_no_alien_weight_v1/best.pt' # model.pt path(s) where is weight?\n",
    "        #source= \"C:\\\\Users\\\\thithilab\\\\Desktop\\\\file\\\\New Data\\\\14\\\\first32\\\\20220310_152525_E1E0.mkv\" # file/dir/URL/glob, 0 for webcam  //change your video path here\n",
    "        source= file_location # file/dir/URL/glob, 0 for webcam  //change your video path here\n",
    "        data='data/coco128.yaml'  # dataset.yaml path\n",
    "        imgsz=(640, 640)  # inference size (height, width)\n",
    "        conf_thres=0.75  # confidence threshold\n",
    "        iou_thres=0.45  # NMS IOU threshold\n",
    "        max_det=1000 # maximum detections per image\n",
    "        device='0'  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        view_img=True  # show results\n",
    "        save_txt=False  # save results to *.txt\n",
    "        save_conf=False  # save confidences in --save-txt labels\n",
    "        save_crop=False  # save cropped prediction boxes\n",
    "        nosave=False  # do not save images/videos\n",
    "        classes=None  # filter by class: --class 0, or --class 0 2 3\n",
    "        agnostic_nms=False  # class-agnostic NMS\n",
    "        augment=False  # augmented inference\n",
    "        visualize=False  # visualize features\n",
    "        update=False  # update all models\n",
    "        project='runs/detect_new'  # save results to project/name\n",
    "        name='exp'  # save results to project/name\n",
    "        exist_ok=False  # existing project/name ok, do not increment\n",
    "        line_thickness=8  # bounding box thickness (pixels)\n",
    "        hide_labels=False  # hide labels\n",
    "        hide_conf=False  # hide confidences\n",
    "        half=False  # use FP16 half-precision inference\n",
    "        dnn=False  # use OpenCV DNN for ONNX inference\n",
    "\n",
    "    return Args()\n",
    "     \n",
    "   #parser here\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    check_requirements(exclude=('tensorboard', 'thop'))\n",
    "    run(**vars(opt))\n",
    "    #run()\n",
    "\n",
    "#__name__==\"__main__\"\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cc499-f330-4968-a479-4417ee596072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73ce4b-15df-4b26-9c96-f38a47d7db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f10cc-ca66-4801-9d45-c8adb2e777ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipkernel_launcher.py --source \"D:\\Python\\env\\Lameness\\Frames\\Videos\\20220201_145508_7108.mp4\"  --yolo-weights weights_slm/best_6_23_gpu.pt --view-img --save-crop --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139acf2-ee9e-4f49-a18a-7e106c993efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d17a6-e24d-40d2-aae9-d7a85fda8c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37758d6-3698-4102-8c29-509297193956",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)  #run me before printing , to find the meaning of variable, you can just find and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f3d53-7d99-41a9-9b25-79fbb3488db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_feature) #print me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b727d-861a-4fbb-922d-389b5bff6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_features)  #print me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21048563-d565-4833-97c1-879c8471da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_For_RF) ## print me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
