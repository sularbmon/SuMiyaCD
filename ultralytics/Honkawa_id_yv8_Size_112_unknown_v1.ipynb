{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b676c638-f48d-49a7-a1a1-6fe98e5b5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thithilab\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "import asyncio\n",
    "import threading\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16abd98a-5a5c-4d05-b271-eedfed0315ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolo->engine->model.py93\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from ultralytics.yolo.utils.files import increment_path\n",
    "import torch\n",
    "import gc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "#from pathlib import Path\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from ultralytics.yolo.utils.plotting import Annotator\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"D:/Python/SULarbmon/Python/env/yolov8_june/runs/segment/Honkawa_Weights_v1/weights/best.pt\")  # load a custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eba0d22-24e9-4695-9ebd-ef41e852f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_filename = '../HONKAWA_VGG_SVM/GENERAL_LE_SUMIYOSHI_1103.le'\n",
    "predictor_filename ='../HONKAWA_VGG_SVM/VGG_SVM_HONKAWA_112_1103_3pfs.pkl'\n",
    "\n",
    "predictor = pickle.load(open(predictor_filename, 'rb'))\n",
    "lable_encoder = pickle.load(open(le_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc46bb71-637a-4054-8201-086685c58da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE=112\n",
    "\n",
    "vgg = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n",
    "\n",
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "for layer in vgg.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "#vgg.summary()  #Trainable parameters will be 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f235b43-c095-4399-b8b5-547abefd452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined predictor\n"
     ]
    }
   ],
   "source": [
    "def Predict_SVM(image):  ## new with vgg\n",
    "    \n",
    "    input_img = np.expand_dims(image, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    input_img_feature=vgg.predict(input_img)\n",
    "    input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\n",
    "    predicted_result = predictor.predict(input_img_features)[0] \n",
    "    predicted_result = lable_encoder.inverse_transform([predicted_result])  #Reverse the label encoder to original name\n",
    "    \n",
    "    return predicted_result\n",
    "print(\"defined predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2d6dec-36da-491e-95ea-896b621be300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### load dataset and model\n",
    "\n",
    "\n",
    "def check_withinROI_NEW(x1,y1,x2,y2,h,w):\n",
    "    #print(x1, '  ',y1, '  ',x2, '  ',y2, '  ',h, '  ',w)\n",
    "    #if(x1<int(X1*(w/default)) or x2>int(X2*(w/default)) or y1<int(Y1_NEW*(h/default)) or y2>int(Y2_NEW*(h/default)) or x1>=int(X2*(w/default))):\n",
    "    #    return False\n",
    "    \n",
    "    if(x1<150 or x2>1750):\n",
    "        return False\n",
    "    if(y2-y1<600 or x2-x1 <250): #1400 to 700 Before\n",
    "        return False\n",
    "    return True  \n",
    "\n",
    "def check_withinROI_Resize(x1,y1,x2,y2,h,w):\n",
    "    #print(x1, '  ',y1, '  ',x2, '  ',y2, '  ',h, '  ',w)\n",
    "    resize_x1=850#*(w/2992)\n",
    "    resize_x2=1050#*(w/2992)\n",
    "    resize_y1=Y1_NEW#Y1_NEW*(w/2992)\n",
    "    resize_y2=Y1_NEW#Y2_NEW*(w/2992)\n",
    "    #print(resize_x1, '  ',resize_y1, '  ',resize_x2, '  ',resize_y2)\n",
    "    if(x1<int(resize_x1) or x2>int(resize_x2) or x1>=int(resize_x2)):\n",
    "        return False\n",
    "    if(y2 - y1>1400 or y2-y1<700): #1400 to 700 Before\n",
    "        return False\n",
    "    return True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056bd001-9ec5-4d37-8d4f-794eb6d030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Is_Duplicate_Id(y1,y2,id):\n",
    "    global PREVIOUS_ID\n",
    "    global PREVIOUS_Y1\n",
    "    global PREVIOUS_Y2\n",
    "    global PREVIOUS_LOCAL_IDS\n",
    "    global CATTLE_LOCAL_ID\n",
    "    \n",
    "    try: \n",
    "        index = PREVIOUS_ID.index(id)\n",
    "        #print('I reached here')\n",
    "        if(PREVIOUS_Y1[index]+321<=y1 and PREVIOUS_Y2[index]+371<y2): #duplicate from bottom\n",
    "         #   if(id in PREVIOUS_LOCAL_IDS):\n",
    "         #   #print('id: ',id,' LOCAL_ID: ',CATTLE_LOCAL_ID)\n",
    "         #       return PREVIOUS_LOCAL_IDS[id][0]\n",
    "            \n",
    "            #print('except')\n",
    "            PREVIOUS_ID.append(CATTLE_LOCAL_ID)\n",
    "            PREVIOUS_Y1.append(y1)\n",
    "            PREVIOUS_Y2.append(y2)\n",
    "            #print('New Cattle Id')\n",
    "            CATTLE_LOCAL_ID+=1\n",
    "            return CATTLE_LOCAL_ID\n",
    "        #elif(PREVIOUS_Y[index]+400<center): #stepping back\n",
    "        #    if(id in PREVIOUS_LOCAL_IDS):\n",
    "        #        return PREVIOUS_LOCAL_IDS[id][0]\n",
    "        else:\n",
    "            #print('Oh. here ? really?')\n",
    "            PREVIOUS_Y1[index]=y1 #duplicate is solved or no duplicate and just need for last y \n",
    "            PREVIOUS_Y2[index]=y2\n",
    "            #return PREVIOUS_LOCAL_IDS[index][1]\n",
    "            \n",
    "            #update('PREVIOUS Y')\n",
    "            return PREVIOUS_ID[index]\n",
    "    except:\n",
    "        #print(PREVIOUS_ID)\n",
    "        #print(id)\n",
    "        CATTLE_LOCAL_ID += 1\n",
    "        #print('except')\n",
    "        PREVIOUS_ID.append(CATTLE_LOCAL_ID)\n",
    "        PREVIOUS_Y1.append(y1)\n",
    "        PREVIOUS_Y2.append(y2)\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed9d4de-4e94-4b9a-a257-9e571cc441ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Take_Prev_Label(y,h,id,cow_srno):\n",
    "    global STORED_IDS\n",
    "    global STORED_MID_Y\n",
    "    global STORED_MID_Y1\n",
    "    global STORED_MID_Y2\n",
    "    global STORED_MISS\n",
    "    global LAST_SEEN_IDS\n",
    "    global LAST_SEEN_ID_CENTROIDS\n",
    "    global CATTLE_LOCAL_ID\n",
    "    global IS_FIRST_CATTLE \n",
    "    y1 , y2 = y , y+h\n",
    "    \n",
    "    if IS_FIRST_CATTLE:\n",
    "        IS_FIRST_CATTLE = False\n",
    "        id = CATTLE_LOCAL_ID\n",
    "    #mid_y = y2\n",
    "    mid_y = int(2*y + h)/2\n",
    "    IS_NEW = True\n",
    "    last_id = 999\n",
    "    last_y1 = 0\n",
    "    last_y2 = 0\n",
    "    if(len(STORED_IDS)>0): \n",
    "        last_id = STORED_IDS[len(STORED_IDS)-1]\n",
    "        last_y1 = STORED_MID_Y1[len(STORED_MID_Y1)-1]#max(STORED_MID_Y1)\n",
    "        last_y2 = STORED_MID_Y2[len(STORED_MID_Y2)-1]#max(STORED_MID_Y2)\n",
    "        MISSED_LEN = len(STORED_MISS)\n",
    "        #if(IS_NEW):\n",
    "        \n",
    "        #    MISSED_LEN -=1\n",
    "        removed = 0\n",
    "        for i in range(MISSED_LEN):\n",
    "            #print(i, ' missed index checking' )\n",
    "            missed = STORED_MISS[i-removed]\n",
    "            #print('checking ',i-removed, 'to remove')\n",
    "            if(missed>70): #if missed 35 frames\n",
    "    \n",
    "                del STORED_MISS[i-removed]  \n",
    "                del STORED_MID_Y[i-removed]\n",
    "                del STORED_MID_Y1[i-removed]\n",
    "                del STORED_MID_Y2[i-removed]\n",
    "                del STORED_IDS[i-removed]\n",
    "                removed+=1\n",
    "                #print('removed')\n",
    "                \n",
    "    #clear misses\n",
    "   \n",
    "    \n",
    "    threshold_1 = 100 #300\n",
    "    threshold_2 = 100  #230\n",
    "    Distance = 5\n",
    "     \n",
    "    #if mid_y <= 1300 or mid_y >= 700:\n",
    "    #    threshold_1 = 320 #350\n",
    "    #    threshold_2 = 370 #280\n",
    "    for i in range(1,len(STORED_MID_Y)+1):\n",
    "        #print(STORED_IDS[-i-1],STORED_MID_Y[-i-1],' ',i)\n",
    "        \n",
    "        \n",
    "        #if(STORED_MID_Y[-i]+threshold_2>=mid_y and STORED_MID_Y[-i]-threshold_1<=mid_y): # and IS_NEW): #previous 150 #200\n",
    "        if(STORED_MID_Y1[-i]-threshold_1<=y1 and STORED_MID_Y1[-i]+threshold_1>=y1) or (STORED_MID_Y2[-i]-threshold_2<=y2 and STORED_MID_Y2[-i]+threshold_2>=y2): # and IS_NEW): #previous 150 #200\n",
    "            if(IS_NEW):\n",
    "              \n",
    "                \n",
    "                Distance = abs(STORED_MID_Y1[-i] - y1)\n",
    "                if(abs(STORED_MID_Y2[-i] - y2)<Distance):\n",
    "                    Distance = abs(STORED_MID_Y2[-i] - y2)\n",
    "                IS_NEW = False\n",
    "                STORED_MID_Y1[-i] = y1\n",
    "                STORED_MID_Y2[-i] = y2\n",
    "                \n",
    "                STORED_MISS[-i]=1\n",
    "                id= STORED_IDS[-i]\n",
    "                #print(Distance)\n",
    "                #print(id)\n",
    "                \n",
    "            #try:\n",
    "            #    exist_index = LAST_SEEN_IDS.index(id)\n",
    "            #    if(LAST_SEEN_ID_CENTROIDS[exist_index]+200>y): # showing old id\n",
    "            #        LAST_SEEN_ID_CENTROIDS[exist_index] = y\n",
    "            #except:\n",
    "            #print('corrected id :',STORED_IDS[-i])\n",
    "            elif Distance >5:\n",
    "                STORED_MISS[-i]+=1\n",
    "            #else:\n",
    "            #    STORED_MISS[-i]-=1 #reset count to 2 when not moving\n",
    "        #elif(STORED_MID_Y1[-i]<=y1 and STORED_MID_Y2[-i]>=y2):\n",
    "        #        STORED_MISS[-i]=30\n",
    "        else:\n",
    "            STORED_MISS[-i]+=1    \n",
    "     \n",
    "    if(IS_NEW == False):\n",
    "        if(y1>last_y1+20 and y2>last_y2+20):\n",
    "            IS_NEW = True\n",
    "        #elif (y1<last_y1+10 and y2<last_y2+10):\n",
    "        #    print('Skipped first here')\n",
    "        #    return -1\n",
    "    \n",
    "        #elif(cow_srno==1):\n",
    "    if(IS_NEW):\n",
    "        CATTLE_LOCAL_ID+=1\n",
    "        id=CATTLE_LOCAL_ID\n",
    "        if(y1<last_y1+70 and y2<last_y2+70):\n",
    "            CATTLE_LOCAL_ID-=1\n",
    "            #print('skipped second here')\n",
    "            for i in range(len(STORED_MID_Y)):\n",
    "                STORED_MISS[i]=5\n",
    "            return -1\n",
    "            \n",
    "        STORED_IDS.append(id)\n",
    "        STORED_MID_Y.append(mid_y)\n",
    "        STORED_MID_Y1.append(y1)\n",
    "        STORED_MID_Y2.append(y2)\n",
    "        STORED_MISS.append(1)\n",
    "    #print(STORED_IDS,' IDS ',STORED_MID_Y,' SMY ',mid_y,' mid_y')\n",
    "    if(IS_NEW) and False:\n",
    "        #print('SMY: ',STORED_MID_Y,', new my:',mid_y) \n",
    "        #print('new id: ',id)\n",
    "        updatedID = Is_Duplicate_Id(y1,y2,id)\n",
    "        if(int(last_id) <int(updatedID) and y1<last_y1-150 and y2<last_y2-150): # duplicate cattle with increased cattleID\n",
    "            CATTLE_LOCAL_ID-=1\n",
    "            for i in range(len(STORED_MID_Y)-1,0,-1):\n",
    "                STORED_MISS[i]=15\n",
    "            return -1\n",
    "        if(int(last_id)-1>int(updatedID)):\n",
    "            return -1\n",
    "            \n",
    "    #if(updatedID!=id):\n",
    "    #    print('orgID: ',id,' updated ID: ',updatedID)\n",
    "        #id = str(updated_ID)+'_'+str(id)\n",
    "        \n",
    "        id=CATTLE_LOCAL_ID\n",
    "        STORED_IDS.append(id)\n",
    "        STORED_MID_Y.append(mid_y)\n",
    "        STORED_MID_Y1.append(y1)\n",
    "        STORED_MID_Y2.append(y2)\n",
    "        STORED_MISS.append(1)\n",
    "    \n",
    "    #print('returned id :',id)\n",
    "    \n",
    "    #print(id)\n",
    "          \n",
    "    result = []\n",
    "    result.append(str(id-1))\n",
    "    \n",
    "    #region remove stored id\n",
    "    removed = 0\n",
    "    #print(STORED_MID_Y1,'  <===== y1, y2 =====>  ',STORED_MID_Y2,'    result =====>',result)\n",
    "    #for i in range(len(STORED_MID_Y)-1,0,-1):\n",
    "    #    if(y1>STORED_MID_Y1[i] and y2>STORED_MID_Y2[i]):\n",
    "    #        del STORED_MISS[i-removed]  \n",
    "    #        del STORED_MID_Y[i-removed]\n",
    "    #        del STORED_MID_Y1[i-removed]\n",
    "    #        del STORED_MID_Y2[i-removed]\n",
    "    #        del STORED_IDS[i-removed]\n",
    "    #        removed+=1\n",
    "                 \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3415efa4-0dc9-4cb9-afd0-ea35f35e1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_THRESHOLD = 0.5\n",
    "cattle_track_count = 0\n",
    "def CALCULATE_MAX_CATTLE_ID(csv_path):\n",
    "    print(csv_path, \" is csv_path and \")\n",
    "    global cattle_track_count\n",
    "    cattle_track_count+=1\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    list_of_csv = [list(row) for row in data.values]\n",
    "\n",
    "    prev_id_record = [] \n",
    "    prev=None\n",
    "\n",
    "    current_cow = []\n",
    "    excel_cow_count = []\n",
    "    #boxes = []\n",
    "    #file_locations = []\n",
    "    total_cattle_count = len(list_of_csv)\n",
    "    for i in range (total_cattle_count):\n",
    "        filtered_id = list_of_csv[i][0]\n",
    "        actual_id = list_of_csv[i][1]\n",
    "        #file_locations.append(list_of_csv[i][2])\n",
    "        #boxes.append([list_of_csv[i][3],list_of_csv[i][4],list_of_csv[i][5],list_of_csv[i][6]])\n",
    "        try: \n",
    "            index = current_cow.index(actual_id)\n",
    "            excel_cow_count[index]+=1\n",
    "        except:\n",
    "            current_cow.append(actual_id)\n",
    "            excel_cow_count.append(1)\n",
    "\n",
    "    maxpos = excel_cow_count.index(max(excel_cow_count)) # max position in cow_count\n",
    "    cattle_id = current_cow[maxpos]  # fetch the cattle id from max position index because they are same index\n",
    "    print('threshold of tracking id:', cattle_track_count , ' is ' , max(excel_cow_count) / total_cattle_count )\n",
    "    if(max(excel_cow_count) / total_cattle_count < UNKNOWN_THRESHOLD):\n",
    "        return -1 #Unknown\n",
    "    return cattle_id#,file_locations,boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27fc9ce9-4de1-47e0-8f0d-bc4ebe83aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_CATTLE = 0\n",
    "def get_final_cattle_id(save_dir,total_cattle):\n",
    "    final_id = []\n",
    "    global UNKNOWN_CATTLE\n",
    "    for i in range(1,total_cattle+1):\n",
    "        csv_path = save_dir + \"/\" + str(i) + \"/\" + str(i) + \".csv\"\n",
    "        \n",
    "        output_id =  CALCULATE_MAX_CATTLE_ID(csv_path)\n",
    "        if output_id == -1:\n",
    "            UNKNOWN_CATTLE+=1\n",
    "            output_id = \"UNKNOWN_\"+str(UNKNOWN_CATTLE)\n",
    "            print('tracking ',i, ' is ',output_id)\n",
    "            \n",
    "        final_id.append(output_id)\n",
    "    return final_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d5d266-6ce4-4300-8274-43cd35ed541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def writeVideo(cap,filePath,csv_name,cattle_ids,total_frame):\n",
    "    img_array = []\n",
    "    size = (1920,1080)\n",
    "    names = ['cow']\n",
    "    main_csv_index = 0\n",
    "    data = pd.read_csv(filePath+\"/\"+csv_name)\n",
    "\n",
    "    list_of_csv = [list(row) for row in data.values]\n",
    "    vid_name = os.path.basename(os.path.normpath(filePath))\n",
    "    total_count = len(list_of_csv)\n",
    "    prev_image_id = 1\n",
    "    #vid_path = str(Path(filePath + \"/\" + vid_name ).with_suffix('.mp4'))\n",
    "    #out = cv2.VideoWriter(vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 6, size)\n",
    "    #if len(img_locations)<10: \n",
    "    #    return -1\n",
    "    for filename in glob.glob(filePath+\"/all_images/\"+'/*.jpg'):\n",
    "        img = cv2.imread(filename)\n",
    "        \n",
    "        #print(os.path.isfile(filename))\n",
    "        #print(filename)\n",
    "        #print(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        img_array.append(img)\n",
    "    image_location = filePath+\"/all_images/\";\n",
    "    img_index = 0\n",
    "    for ind in range(1,total_frame):\n",
    "        image_path = image_location + str(ind) +\".jpg\"\n",
    "        #print(image_path)\n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "        #print(os.path.isfile(filename))\n",
    "        #print(filename)\n",
    "        #print(filename)\n",
    "        #height, width, layers = img.shape\n",
    "        #size = (width,height)\n",
    "        #img = img_array[img_index]\n",
    "        #img_index +=1\n",
    "        #img = cv2.resize(img, size, interpolation = cv2.INTER_AREA)\n",
    "        #print(img.Shape)\n",
    "        #cv2.imshow('Cattle Images ', img)\n",
    "        #if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        #    break\n",
    "        annotator = Annotator(img, line_width=3, example=str(names)) #font here\n",
    "        from_index = main_csv_index\n",
    "        print(from_index, ' from index')\n",
    "        is_draw = False\n",
    "        for i in range(from_index,total_count-1):\n",
    "            \n",
    "            main_csv_index+=1\n",
    "            print(ind, \"  <--x-->  \", list_of_csv[i][0])\n",
    "            if(ind!=list_of_csv[i][0]):\n",
    "                main_csv_index-=1\n",
    "                break\n",
    "            print('SAVING!!!!!')\n",
    "            tracking_id  = list_of_csv[i][1] -1\n",
    "            xyxy = [list_of_csv[i][2],list_of_csv[i][3],list_of_csv[i][4],list_of_csv[i][5]]\n",
    "\n",
    "            try:\n",
    "                annotator.box_label(xyxy,str(cattle_ids[tracking_id]), color=(15, 0, 255))\n",
    "            except:\n",
    "                annotator.box_label(xyxy,cattle_ids[tracking_id]+\"_\"+str(tracking_id), color=(15, 0, 255))\n",
    "            is_draw = True\n",
    "        if is_draw:\n",
    "            annotated_img =cv2.resize(annotator.result(),size) \n",
    "            cap.write(annotated_img)\n",
    "            #except:\n",
    "            #    print('did not write')\n",
    "            #    continue\n",
    "    #ut.release()\n",
    "    img_array=[]\n",
    "    print(\"done \", vid_name)\n",
    "    #v2.destroyAllWindows()\n",
    "    return id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c55fddc-ffc5-432b-bd34-c94d6d711c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw a bounding box and annotate the image\n",
    "def draw_bounding_box(image, box, label):\n",
    "    # Extract the coordinates from the box\n",
    "    x1, y1, x2, y2 = box\n",
    "    #print(x1,' ',y1,x2,y2)\n",
    "\n",
    "    # Draw the bounding box rectangle on the image\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "\n",
    "    # Define the text properties\n",
    "    text = f'{label}'\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale =1\n",
    "    thickness = 2\n",
    "\n",
    "    # Calculate the size of the text\n",
    "    (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "\n",
    "    # Calculate the position for placing the text\n",
    "    text_x = x1\n",
    "    text_y = y1 - 10 if y1 >= 20 else y1 + 10 + text_height\n",
    "\n",
    "    # Draw the text background rectangle\n",
    "    cv2.rectangle(image, (text_x, text_y - text_height - 5), (text_x + text_width, text_y), (0, 255, 0), -1)\n",
    "\n",
    "    # Put the label text on the image\n",
    "    cv2.putText(image, text, (text_x, text_y), font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8cb6e8e-04e2-40ca-bd07-db6d380a4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(image, mask, color, alpha, resize=None):\n",
    "    \"\"\"Combines image and its segmentation mask into a single image.\n",
    "    \n",
    "    Params:\n",
    "        image: Training image. np.ndarray,\n",
    "        mask: Segmentation mask. np.ndarray,\n",
    "        color: Color for segmentation mask rendering.  tuple[int, int, int] = (255, 0, 0)\n",
    "        alpha: Segmentation mask's transparency. float = 0.5,\n",
    "        resize: If provided, both image and its mask are resized before blending them together.\n",
    "        tuple[int, int] = (1024, 1024))\n",
    "\n",
    "    Returns:\n",
    "        image_combined: The combined image. np.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    # color = color[::-1]\n",
    "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
    "    colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
    "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
    "    image_overlay = masked.filled()\n",
    "\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
    "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
    "\n",
    "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
    "\n",
    "    return image_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3183ce0c-9532-4cee-9232-15a5689fb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_LEFT_TO_RIGHT_ORDER(x1s):\n",
    "    orders = []\n",
    "    size = len(x1s)\n",
    "    #print(' x1s     ',x1s)\n",
    "    clones = x1s[:]\n",
    "    for i in range(size):\n",
    "        #print(clones)\n",
    "        id = (x1s.index(min(clones)))\n",
    "        #print(id)\n",
    "        orders.append(id)\n",
    "        clones.pop(clones.index(min(clones)))\n",
    "    #print(' sorted order :',orders)\n",
    "    return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46de810b-c4a0-4435-bb1c-abf138ec20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add_lines_to_excel(existing_file, data_to_add):\n",
    "    \"\"\"\n",
    "    Add lines (rows) to an existing Excel file using a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        existing_file (str): Path to the existing Excel file.\n",
    "        data_to_add (dict): Dictionary containing data to add. Keys are column names,\n",
    "                           and values are lists of data for each column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_existing = pd.read_csv(existing_file,dtype={\n",
    "    'ImageId': 'string',\n",
    "    'LocalId': np.int64,\n",
    "    'xyxy1': np.int64,\n",
    "    'xyxy2': np.int64,\n",
    "    'xyxy3': np.int64,\n",
    "    'xyxy4': np.int64\n",
    "})\n",
    "    except FileNotFoundError:\n",
    "        df_existing = pd.DataFrame()\n",
    "\n",
    "    df_to_add = pd.DataFrame(data_to_add)\n",
    "    df_combined = pd.concat([df_existing, df_to_add], ignore_index=True)\n",
    "    df_combined.to_csv(existing_file, index=False)\n",
    "\n",
    "\n",
    "#add_lines_to_excel(existing_excel_file, data_to_add)\n",
    "#This function encapsulates the process of adding lines to an existing Excel file using a DataFrame. It first attempts to read the existing Excel file, creating an empty DataFrame if the file doesn't exist. Then, it creates a DataFrame from the data to add and concatenates it with the existing DataFrame. Finally, it writes the combined DataFrame back to the Excel file.\n",
    "\n",
    "#Make sure to replace 'path/to/your/existing/excel/file.xlsx' with the actual path to your existing Excel file and adjust the data_to_add dictionary accordingly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ac11d5-826d-473a-aa0f-a76241b4dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_csv(csv_file_path):\n",
    "    default_columns = ['ImageId', 'LocalId', 'xyxy1', 'xyxy2', 'xyxy3', 'xyxy4']  # Add your desired column headers here\n",
    "\n",
    "    # Create an empty DataFrame with the default columns\n",
    "    empty_df = pd.DataFrame(columns=default_columns)\n",
    "\n",
    "    # Save the empty DataFrame to a CSV file\n",
    "    empty_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c82f8d-36b2-42e0-9809-a7c23c993f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_main_csv(data):\n",
    "    with main_df_lock:\n",
    "        global MAIN_DF\n",
    "        MAIN_DF = pd.concat([MAIN_DF, data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7045a-10a2-4a01-b48e-19a11473fbc7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X1=200 #same as NEW_BLACK_X1\n",
    "X2=400 #same as NEW_BLACK_X2 # incase of x2 out of bound\n",
    "Y1=120\n",
    "Y2=500\n",
    "\n",
    "Y1_NEW=120 #125  #decrease here to extend, increase to shrink \n",
    "Y2_NEW=510  #500  # redyce here to extend , increase to do vice casa 460 previous\n",
    "FRAME = 1\n",
    "\n",
    "default=640\n",
    "\n",
    "BATCH = 100\n",
    "BATCH_COUNT = 1\n",
    "PREV_BATCH = 0\n",
    "\n",
    "LAST_SEEN = time.time()\n",
    "FIRST_SEEN = True\n",
    "demo_img_save_path = []\n",
    "main_df_lock = threading.Lock()\n",
    "\n",
    "\n",
    "#end\n",
    "prevId_record =[]\n",
    "MAX_prevId = [] \n",
    "MAX_xyxy1 = [] \n",
    "MAX_xyxy2 = [] \n",
    "MAX_xyxy3 = [] \n",
    "MAX_xyxy4 = [] \n",
    "MAX_orgId = []\n",
    "IMAGE_STORED_LOCATION = []\n",
    "IMAGE_ID_LIST = []\n",
    "LOCAL_ID_LIST = []\n",
    "#end\n",
    "#prevId_record =[]\n",
    "TOTAL_CATTLE_COUNT = 0 \n",
    "\n",
    "\n",
    "#end\n",
    "\n",
    "#region Cattle Tracking\n",
    "STORED_IDS= []\n",
    "STORED_MID_Y = []\n",
    "STORED_MID_Y1 = []\n",
    "STORED_MID_Y2 = []\n",
    "STORED_MISS = []\n",
    "PREVIOUS_ID = [] # keep the record of last seen ids and position\n",
    "PREVIOUS_Y1 = [] \n",
    "PREVIOUS_Y2 = [] \n",
    "PREVIOUS_LOCAL_IDS = []\n",
    "CATTLE_LOCAL_ID= 1\n",
    "IS_FIRST_CATTLE = True\n",
    "\n",
    "UNKNOWN_CATTLE = 0\n",
    "start_time = time.time()\n",
    "\n",
    "MAIN_DF_COLUMNS = ['ImageId',\t'LocalId',\t'xyxy1',\t'xyxy2',\t'xyxy3',\t'xyxy4'] # using to store all cattle information in here instead of list\n",
    "MAIN_DF = pd.DataFrame(columns = MAIN_DF_COLUMNS)\n",
    "\n",
    "# Predict with the model\n",
    "project = 'D:/Python/SULarbmon/Python/env/yolov8_june/ultralytics/runs/segment/honkawa/1103_112_3fps_03'\n",
    "name = 'identification'\n",
    "dataset = \"D:\\\\815_CowDataChecking\\\\Honkawa\\\\2023-07-31\\\\0304_output_3fps\"  #0304_output_3fps\n",
    "#save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mkv','_track')  #open this when running single video\n",
    "save_vid_name = dataset.split(\"\\\\\")[-1]+\"_track\" # open this when running multiple videos\n",
    "\n",
    "print(save_vid_name)\n",
    "results = model(dataset,imgsz=(960,640),save=False,retina_masks=False,show=False,stream=True,device='0',conf=0.3)\n",
    "#results = model('D:\\\\815_CowDataChecking\\\\20221228\\\\20221228_E_cow\\\\20221228_151533_D474.mkv',imgsz=1088,save=False,retina_masks=False,show=False,stream=True,device='0',conf=0.2)\n",
    "#save_dir = increment_path(Path(project) / name, exist_ok=True)  # increment run\n",
    "save_dir = increment_path(Path(project) / name,mkdir=True)\n",
    "csv_main_file_path = str(save_dir) + \"\\main_csv.csv\"\n",
    "#create_default_csv(csv_main_file_path)\n",
    "#(save_dir / 'labels' if False else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "manual_cow_count = 0\n",
    "\n",
    "#cap.set(4, 480)\n",
    "save_vid_path = str(Path(os.path.join(save_dir, save_vid_name)).with_suffix('.mp4'))\n",
    "print(save_vid_path)\n",
    "cap = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 30, (960,640))\n",
    "skip_count = 2\n",
    "skip_track = 1\n",
    "image_count = 1\n",
    "\n",
    "SAVING_THRESHOLD =200\n",
    "SAVED_COUNT = 1\n",
    "\n",
    "CATTLE_SAVING_THRESHOLD = 15\n",
    "CATTLE_SAVED_COUNT = 0\n",
    "last_tracking_id = 0\n",
    "for result in results:\n",
    "    if(skip_track==2) and False:\n",
    "        skip_track = 1\n",
    "        continue\n",
    "    #skip_track+=1\n",
    "    \n",
    "    #print(result.boxes)\n",
    "    vid_path = result.path\n",
    "    filename = vid_path.split(\"\\\\\")[-1].replace(\".mp4\",\"\")\n",
    "    \n",
    "    boxes = result.boxes.cpu().numpy()\n",
    "    detections = boxes.xyxy.tolist()\n",
    "    #print(detections)\n",
    "    # Sort the detections based on the x1 coordinate (i.e., left-to-right)\n",
    "    #detections.sort(key=lambda x: x[0])\n",
    "    left_to_right = GET_LEFT_TO_RIGHT_ORDER([t[0] for t in detections])\n",
    "    #print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "    #print(left_to_right)\n",
    "    #print(detections)\n",
    "    \n",
    "    ori_img = cv2.resize(result.orig_img, (640,384), interpolation = cv2.INTER_AREA)\n",
    "    annotator = Annotator(ori_img)\n",
    "    box_count = 0\n",
    "    cow_position = 1\n",
    "    \n",
    "    h,w = result.orig_shape\n",
    "    count = 1\n",
    "    b_boxes = []\n",
    "    masks = []\n",
    "    ids = []\n",
    "    has_cattle = False\n",
    "    \n",
    "    if  result.masks != None:\n",
    "        \n",
    "        #result_masks = result.masks.cpu().numpy().masks.astype(bool)\n",
    "        #for m in result.masks.cpu().numpy().masks.astype(bool):\n",
    "        result_masks = result.masks.cpu().numpy().masks.astype(bool)\n",
    "        \n",
    "        for LR_index in left_to_right:\n",
    "            m = result_masks[LR_index]\n",
    "        #for i in range(1,len(result_masks)+1):\n",
    "        #for m in result.masks.masks.astype(bool):\n",
    "            \n",
    "            xyxy = boxes[LR_index].xyxy[0]\n",
    "            #print('LR  INDEX  ',LR_index)\n",
    "            #print(xyxy)\n",
    "        #   m = result_masks[-1]\n",
    "            #print(xyxy)\n",
    "            #print(m.shape)\n",
    "            box_count += 1\n",
    "            x1= int(xyxy[0])\n",
    "            y1= int(xyxy[1])\n",
    "            x2= int(xyxy[2])\n",
    "            y2= int(xyxy[3])\n",
    "            #print(xyxy)\n",
    "            #print(m.shape)\n",
    "            \n",
    "            ################## Validate  #####################\n",
    "            if(check_withinROI_NEW(x1,y1,x2,y2,h,w)==False):\n",
    "                #print(\"I was skipped\")\n",
    "                continue\n",
    "            #print(\"not skipped\")\n",
    "            has_cattle = True\n",
    "            #new_results.append(result)\n",
    "            box_left = x1\n",
    "            box_top = y1\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "\n",
    "            new = np.zeros_like(ori_img, dtype=np.uint8)\n",
    "            new[m] = ori_img[m]\n",
    "            \n",
    "           \n",
    "            x1= int(x1 * (640/1920))\n",
    "            x2= int(x2 * (640/1920))\n",
    "            y1= int(y1 * (384/1080))\n",
    "            y2= int(y2 * (384/1080))\n",
    "\n",
    "            crop = new[y1:y2, x1:x2]\n",
    "            img = cv2.resize(crop, (SIZE, SIZE))\n",
    "            img=img / 255.0\n",
    "            \n",
    "            ############# LABLE\n",
    "            label = Predict_SVM(img)\n",
    "            \n",
    "            \n",
    "            ###### LABEL\n",
    "            #img = cv2.resize(crop, (SIZE, SIZE))\n",
    "            #img=img / 255.0\n",
    "            #print(box_left,'    xxxxxx     ' ,box_w)\n",
    "            prev_id = Take_Prev_Label(box_left,box_w,label,cow_position) ## just passing x values instead of y\n",
    "          #########################################  \n",
    "            HAS_COW=True\n",
    "            #has_cattle = True\n",
    "            if(prev_id==-1): #skip cattle when prev_id // filter id is -1\n",
    "                if(count==1):\n",
    "                    has_seen_cattle=False\n",
    "                    count-=1\n",
    "                #print('skipped')\n",
    "                continue\n",
    "            has_cattle = True\n",
    "            \n",
    "            ids.append(prev_id[0])\n",
    "            last_tracking_id = int(prev_id[0])\n",
    "            masks.append(m)\n",
    "            b_boxes.append([x1,y1,x2,y2])\n",
    "            #print(prev_id)\n",
    "            cow_position+=1 \n",
    "            BATCH_COUNT = prev_id[0] # skip batch count here  \n",
    "            \n",
    "           \n",
    "            if CATTLE_SAVED_COUNT > CATTLE_SAVING_THRESHOLD:\n",
    "                print(\"SAVING CATTLE!!!!!!!!!!!!!!!!!!   SAVED_COUNT \",SAVED_COUNT , \" > \", CATTLE_SAVING_THRESHOLD)\n",
    "                for i in range(8):\n",
    "                    tracking_id = MAX_prevId[0][0]\n",
    "                    df = pd.DataFrame(MAX_prevId[0], columns = ['ID'])\n",
    "                    print(\"saving max_orgId csv :\", len(MAX_orgId[0]), ' Tracking ID is :', tracking_id)\n",
    "                    try:\n",
    "                        org_ids = torch.tensor(MAX_orgId[0], device = 'cpu')\n",
    "                        df[\"Original\"] = org_ids\n",
    "                    except:\n",
    "                         df[\"Original\"] = MAX_orgId[0]\n",
    "                    \n",
    "                    del MAX_orgId[0]\n",
    "                    del MAX_prevId[0]\n",
    "                    del prevId_record[0]\n",
    "                    \n",
    "                    save_csv_each_path = str(Path(save_dir / str(tracking_id) / f'{str(tracking_id)}.csv'))\n",
    "                    df.to_csv(save_csv_each_path, index= False)##asdfasdf\n",
    "                CATTLE_SAVED_COUNT = len(MAX_prevId)\n",
    "                #gc.collect()\n",
    "\n",
    "                #torch.cuda.empty_cache()\n",
    "            ###################### CREATE dir to save img\n",
    "            #print(prev_id)\n",
    "            base_path = str(Path(save_dir / prev_id[0]))\n",
    "            if not os.path.exists(base_path):\n",
    "                os.makedirs(base_path)\n",
    "\n",
    "\n",
    "            #demo_annotated_img_save_path = Path(base_path+ '/' + f'{image_count}.jpg')\n",
    "            CATTLE_DATA = {}\n",
    "            try:\n",
    "                \n",
    "                index_prevId = prevId_record.index(int(prev_id[0]))\n",
    "                #print(index_prevId)\n",
    "                MAX_prevId[index_prevId].append(int(prev_id[0]))#,int(label[0]),xyxy)\n",
    "                \n",
    "                MAX_orgId[index_prevId].append(int(label[0]))\n",
    "\n",
    "                #IMAGE_STORED_LOCATION[index_prevId].append(demo_annotated_img_save_path)\n",
    "\n",
    "            except :\n",
    "                CATTLE_SAVED_COUNT += 1\n",
    "                TOTAL_CATTLE_COUNT +=1 \n",
    "                print('new cattle  xxxxxxxxxxxxxxxxxxxxx new cattle alert ' )\n",
    "                prevId_record.append(int(prev_id[0]))\n",
    "                # print(len(prevId_record)-1, 'prevID_record ', len(MAX_prevId) , 'max_previd' )\n",
    "\n",
    "                #MAX_prevId[len(prevId_record)-1].append(int(prev_id[0]))#,int(label[0]),xyxy)\n",
    "                #MAX_xyxy[len(MAX_prevId)-1].append(xyxy)\n",
    "                #MAX_orgId[len(MAX_prevId)-1].append(int(label[0]))\n",
    "                MAX_prevId.append([int(prev_id[0])])#,int(label[0]),xyxy)\n",
    "\n",
    "            \n",
    "                MAX_orgId.append([int(label[0])])\n",
    "                #IMAGE_STORED_LOCATION.append([demo_annotated_img_save_path])\n",
    "            IMAGE_ID_LIST.append(image_count)\n",
    "            LOCAL_ID_LIST.append(int(prev_id[0]))\n",
    "            MAX_xyxy1.append(x1)\n",
    "            MAX_xyxy2.append(y1)\n",
    "            MAX_xyxy3.append(x2)\n",
    "            MAX_xyxy4.append(y2)\n",
    "            \n",
    "            #try:\n",
    "                #demo_vid_index = demo_vid_path.index(demo_path)\n",
    "            #    demo_vid_index = demo_img_save_path.index(base_path)\n",
    "\n",
    "                #print(\"path exist\")\n",
    "            #except:\n",
    "                #manual_summarize_ids.append(int(prev_id[0]))\n",
    "                #manual_local_ids.append(manual_id)\n",
    "                #manual_id +=1\n",
    "            #    demo_img_save_path.append(base_path)\n",
    "                \n",
    "            try:\n",
    "                ori_img = overlay(ori_img,m,(0,0,255),0.3)\n",
    "                #cv2.imwrite(str(demo_annotated_img_save_path), ori_img) #save ori_img\n",
    "                #print(demo_annotated_img_save_path)\n",
    "            except:\n",
    "                print('cannot save ',demo_annotated_img_save_path)\n",
    "            #change cropped size here  #230 to 215 410 to 390\n",
    "\n",
    "            manual_cow_count += 1\n",
    "\n",
    "            manual_cow_count += 1\n",
    "\n",
    "    #frame = annotator.result() \n",
    "   \n",
    "    \n",
    "    \n",
    "    frame = cv2.resize(ori_img, (1920,1080), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    if has_cattle:  \n",
    "        print(\"total saved cattle count -=>\",CATTLE_SAVED_COUNT)\n",
    "        data_to_add = {\n",
    "        'ImageId': IMAGE_ID_LIST,\n",
    "        'LocalId': LOCAL_ID_LIST,\n",
    "        'xyxy1': MAX_xyxy1,\n",
    "        'xyxy2': MAX_xyxy2,\n",
    "        'xyxy3': MAX_xyxy3,\n",
    "        'xyxy4': MAX_xyxy4\n",
    "        # Add more columns as needed\n",
    "        }\n",
    "        df_to_add = pd.DataFrame(data_to_add)\n",
    "        #MAIN_DF = pd.concat([MAIN_DF,df_to_add], ignore_index=True)\n",
    "        csv_thread = threading.Thread(target=concat_main_csv, args=(df_to_add,))\n",
    "        #csv_thread.start()\n",
    "        \n",
    "        base_path = str(Path(save_dir / 'all_images'))\n",
    "        if not os.path.exists(base_path):\n",
    "            os.makedirs(base_path)\n",
    "        del IMAGE_ID_LIST[:]\n",
    "        del LOCAL_ID_LIST[:]\n",
    "        del MAX_xyxy1[:]\n",
    "        del MAX_xyxy2[:]\n",
    "        del MAX_xyxy3[:]\n",
    "        del MAX_xyxy4[:]\n",
    "\n",
    "        demo_annotated_img_save_path = Path(base_path+ '/' + f'{image_count}.jpg')\n",
    "        #cv2.imwrite(str(demo_annotated_img_save_path), ori_img) #save ori_img\n",
    "        save_thread = threading.Thread(target=cv2.imwrite, args=(str(demo_annotated_img_save_path),ori_img))\n",
    "        save_thread.start()\n",
    "        SAVED_COUNT += 1\n",
    "        image_count += 1\n",
    "        \n",
    "        #ori_img = cv2.resize(ori_img,(1088,1088))\n",
    "        for i in range(len(b_boxes)):\n",
    "            box = b_boxes[i]\n",
    "            mask = masks[i]\n",
    "            #print(\"drawing\")\n",
    "            #print(ids)\n",
    "            \n",
    "            draw_bounding_box(ori_img,(box[0],box[1],box[2],box[3]),str(ids[i]))\n",
    "            ori_img = overlay(ori_img,mask,(0,0,255),0.3)\n",
    "        frame = cv2.resize(ori_img, (960,640), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        video_thread = threading.Thread(target=cap.write(frame))\n",
    "        video_thread.start()\n",
    "        del ids[:]\n",
    "        \n",
    "        if threading.active_count() > 100:\n",
    "            pass\n",
    "        \n",
    "    cv2.imshow('YOLO V8 Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "cap.release() #tracking video\n",
    "cv2.destroyAllWindows()\n",
    "image_count -=1 \n",
    "\n",
    "#while threading.active_count() > 1:\n",
    "pass\n",
    "\n",
    "###### write final csv\n",
    "for csv_index in range(len(MAX_prevId)):\n",
    "    df = pd.DataFrame(MAX_prevId[csv_index] , columns = ['ID'])\n",
    "    tracked_id = MAX_prevId[csv_index][0]\n",
    "    try:\n",
    "        org_ids = torch.tensor(MAX_orgId[csv_index], device = 'cpu')\n",
    "        df[\"Original\"] = org_ids\n",
    "    except:\n",
    "        df[\"Original\"] = MAX_orgId[csv_index]\n",
    "    now=str(datetime.now().date())\n",
    "\n",
    "    save_csv_each_path = str(Path(save_dir / str(tracked_id) / f'{str(tracked_id)}.csv'))\n",
    "    df.to_csv(save_csv_each_path, index= False)##asdfasdf\n",
    "#All records     \n",
    "\n",
    "\n",
    "IMAGE_STORED_LOCATION = []\n",
    "\n",
    "cattle_ids = []\n",
    "#################################\\\n",
    "manual_summarize_ids = []\n",
    "manual_local_ids = []\n",
    "\n",
    "manual_summarize_ids = []\n",
    "manual_local_ids = []\n",
    "#### write video after saving csv\n",
    "final_cattle_count = 1\n",
    "\n",
    "# SAVING MAIN CSV\n",
    "MAIN_DF.to_csv(csv_main_file_path, index=False)\n",
    "\n",
    "MAIN_DF = None # CLEAR AFTER SAVING\n",
    "\n",
    "\n",
    "save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mp4','')+'_classification'\n",
    "\n",
    "save_vid_path = str(Path(os.path.join(save_dir, save_vid_name)).with_suffix('.mp4'))\n",
    "print(save_vid_path)\n",
    "classification_vid = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 15, (640,384))\n",
    "print(save_vid_path)\n",
    "\n",
    "\n",
    "#SAVING SUMMARIZE IDS\n",
    "manual_summarize_ids = get_final_cattle_id(str(save_dir),TOTAL_CATTLE_COUNT)\n",
    "\n",
    "summarize_id_csv = pd.DataFrame(manual_summarize_ids, columns = [\"Cow Id\"])\n",
    "\n",
    "summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_'+now+'.csv', index= False) \n",
    "#FINISH SAVING SUMMARIZE IDS\n",
    "\n",
    "csv_name = 'main_csv.csv'\n",
    "#region Write Video\n",
    "try:\n",
    "    writeVideo(classification_vid,str(save_dir),csv_name ,manual_summarize_ids,image_count)\n",
    "except:\n",
    "    print(\" I am ugly\")\n",
    "#summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_'+now+'.csv', index= False) \n",
    "classification_vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04772ad2-2ea1-4b33-8085-ea47413eb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Total Prediction Duration for 0304 AM: \",elapsed_time)\n",
    "#run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c18b4-ca98-4c29-ab9d-04847c722e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_index in range(len(MAX_prevId)):\n",
    "    df = pd.DataFrame(MAX_prevId[csv_index] , columns = ['ID'])\n",
    "    tracked_id = MAX_prevId[csv_index][0]\n",
    "    try:\n",
    "        org_ids = torch.tensor(MAX_orgId[csv_index], device = 'cpu')\n",
    "        df[\"Original\"] = org_ids\n",
    "    except:\n",
    "        df[\"Original\"] = MAX_orgId[csv_index]\n",
    "    now=str(datetime.now().date())\n",
    "\n",
    "    save_csv_each_path = str(Path(save_dir / str(tracked_id) / f'{str(tracked_id)}.csv'))\n",
    "    df.to_csv(save_csv_each_path, index= False)##asdfasdf\n",
    "MAIN_DF.to_csv(csv_main_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82466796-6a15-4845-a1a4-0c6fac562e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_main_file_path)\n",
    "csv_main_file_path = 'D:\\\\Python\\\\SULarbmon\\\\Python\\\\env\\\\yolov8_june\\\\ultralytics\\\\runs\\\\segment\\\\honkawa\\\\291_unknown\\\\identification4\\\\main_csv.csv'\n",
    "MAIN_DF.to_csv(csv_main_file_path, index=False)\n",
    "print('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837f6dd-1886-4aad-8d3a-a0082f55a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "project = 'D:\\\\Honkawa_Exe_output\\\\identification2_0805'\n",
    "name = 'identification_new2'\n",
    "dataset = \"D:\\\\815_CowDataChecking\\\\Honkawa\\\\2023-08-04\\\\03\\\\0304\" \n",
    "#save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mkv','_track')  #open this when running single video\n",
    "\n",
    "\n",
    "#results = model('D:\\\\815_CowDataChecking\\\\20221228\\\\20221228_E_cow\\\\20221228_151533_D474.mkv',imgsz=1088,save=False,retina_masks=False,show=False,stream=True,device='0',conf=0.2)\n",
    "#save_dir = increment_path(Path(project) / name, exist_ok=True)  # increment run\n",
    "save_dir = Path(project,name)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687720f3-3757-43b4-9817-c5d7d6216022",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save_dir = \"D:\\\\Python\\\\SULarbmon\\\\Python\\\\env\\\\yolov8_june\\\\ultralytics\\\\runs\\\\segment\\\\honkawa\\\\1103\\\\identification_new2\"\n",
    "UNKNOWN_THRESHOLD = 0.5\n",
    "cattle_track_count = 0\n",
    "UNKNOWN_CATTLE = 0\n",
    "\n",
    "\n",
    "save_dir = \"D:\\\\Python\\\\SULarbmon\\\\Python\\\\env\\\\yolov8_june\\\\ultralytics\\\\runs\\\\segment\\\\honkawa\\\\291_unknown\\\\identification4\"\n",
    "csv_main_file_path = save_dir + \"\\main_csv.csv\"\n",
    "dataset =  \"D:\\\\815_CowDataChecking\\\\Honkawa\\\\2023-07-31\\\\03\" \n",
    "TOTAL_CATTLE_COUNT=255\n",
    "\n",
    "now=str(datetime.now().date())\n",
    "\n",
    "save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mp4','')+'_classification'\n",
    "\n",
    "save_vid_path = str(Path(os.path.join(save_dir, save_vid_name)).with_suffix('.mp4'))\n",
    "print(save_vid_path)\n",
    "classification_vid = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 15, (640,384))\n",
    "print(save_vid_path)\n",
    "#### write video after saving csv\n",
    "manual_summarize_ids = get_final_cattle_id(str(save_dir),TOTAL_CATTLE_COUNT)\n",
    "summarize_id_csv = pd.DataFrame(manual_summarize_ids, columns = [\"Cow Id\"])\n",
    "summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_50_percent_'+now+'.csv', index= False) \n",
    "\n",
    "csv_name = 'main_csv.csv'\n",
    "#region Write Video\n",
    "image_count = 16201\n",
    "try:\n",
    "    writeVideo(classification_vid,str(save_dir),csv_name ,manual_summarize_ids,image_count)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "#summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_'+now+'.csv', index= False) \n",
    "classification_vid.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ebfe1-b310-4d49-9376-cd19b2896c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a327c40-672c-4b32-b442-ab6f585eaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_'+now+'.csv', index= False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbc49c-cfb5-46cf-a5f7-3eda38622a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee745d-1e26-4f99-aeb7-a454fce0698c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78864315-f639-44bd-8c69-6a9454f9e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"D:\\\\815_CowDataChecking\\\\20220704\\\\20220704_M_Cow\" #don't add \\\\ at the end\n",
    "#save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mkv','_track')  #open this when running single video\n",
    "save_vid_name = dataset.split(\"\\\\\")[-1] # open this when running multiple videos\n",
    "print(save_vid_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe5233-4b20-4570-a9fc-312d3a591930",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "        \n",
    "\n",
    "###### write final csv\n",
    "for csv_index in range(len(MAX_prevId)):\n",
    "    df = pd.DataFrame(MAX_prevId[csv_index] , columns = ['ID'])\n",
    "    tracked_id = MAX_prevId[csv_index][0]\n",
    "    try:\n",
    "        org_ids = torch.tensor(MAX_orgId[csv_index], device = 'cpu')\n",
    "        df[\"Original\"] = org_ids\n",
    "    except:\n",
    "        df[\"Original\"] = MAX_orgId[csv_index]\n",
    "    now=str(datetime.now().date())\n",
    "\n",
    "    save_csv_each_path = str(Path(save_dir / str(tracked_id) / f'{str(tracked_id)}.csv'))\n",
    "    df.to_csv(save_csv_each_path, index= False)##asdfasdf\n",
    "#All records     \n",
    "data_to_add = {\n",
    "    'ImageId': IMAGE_ID_LIST,\n",
    "    'LocalId': LOCAL_ID_LIST,\n",
    "    'xyxy1': MAX_xyxy1,\n",
    "    'xyxy2': MAX_xyxy2,\n",
    "    'xyxy3': MAX_xyxy3,\n",
    "    'xyxy4': MAX_xyxy4\n",
    "    # Add more columns as needed\n",
    "}\n",
    "add_lines_to_excel(csv_main_file_path, data_to_add)\n",
    "\n",
    "IMAGE_STORED_LOCATION = []\n",
    "\n",
    "cattle_ids = []\n",
    "#################################\\\n",
    "manual_summarize_ids = []\n",
    "manual_local_ids = []\n",
    "\n",
    "manual_summarize_ids = []\n",
    "manual_local_ids = []\n",
    "#### write video after saving csv\n",
    "final_cattle_count = 1\n",
    "\n",
    "\n",
    "\n",
    "save_vid_name=  dataset.split(\"\\\\\")[-1].replace('.mkv','')+'_classification'\n",
    "save_vid_path = str(Path(os.path.join(save_dir, save_vid_name)).with_suffix('.mp4'))\n",
    "classification_vid = cv2.VideoWriter(save_vid_path,cv2.VideoWriter_fourcc(*'mp4v'), 13, (1920,1080))\n",
    "\n",
    "#manual_summarize_ids = get_final_cattle_id(save_dir,CATTLE_SAVED_COUNT)\n",
    "#for loc in range(len(demo_img_save_path)):\n",
    "#    print(demo_img_save_path[loc])\n",
    "#    final_cattle_id = writeVideo(classification_vid,demo_img_save_path[loc])\n",
    "#    if(final_cattle_id != -1):\n",
    "#        manual_local_ids.append(final_cattle_count)\n",
    "#        manual_summarize_ids.append(final_cattle_id)\n",
    "#        final_cattle_count+=1 \n",
    "\n",
    "\n",
    "\n",
    "print(save_vid_path)\n",
    "#### write video after saving csv\n",
    "manual_summarize_ids = get_final_cattle_id(str(save_dir),TOTAL_CATTLE_COUNT)\n",
    "\n",
    "summarize_id_csv = pd.DataFrame(manual_summarize_ids, columns = [\"Cow Id\"])\n",
    "\n",
    "summarize_id_csv.to_csv(str(save_dir)+'/summarize_id_'+now+'.csv', index= False) \n",
    "csv_name = 'main_csv.csv'\n",
    "#region Write Video\n",
    "try:\n",
    "    writeVideo(classification_vid,str(save_dir),csv_name ,manual_summarize_ids,image_count)\n",
    "except:\n",
    "    print(\" Done\")\n",
    "\n",
    "classification_vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "#prevId_record = []\n",
    "#MAX_prevId = []\n",
    "#MAX_xyxy1 = [] \n",
    "#MAX_xyxy2 = [] \n",
    "#MAX_xyxy3 = [] \n",
    "#MAX_xyxy4 = [] \n",
    "#MAX_orgId = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100c5ea-47e4-4ddb-83a7-24e17f704c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
